
---

# Loss Function in Deep Learning

---

![Alt text](img/image-52.png)


`loss function ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ measure  ‡¶ï‡¶∞‡¶ø ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ï‡¶§ ‡¶≠‡¶æ‡¶≤‡ßã ‡¶¨‡¶æ ‡¶ï‡¶§ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡•§ ‡¶Ø‡¶¶‡¶ø  loss function ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶ñ‡¶æ‡¶∞‡¶æ‡¶™ ‡¶Ü‡¶∞ ‡¶Ø‡¶¶‡¶ø ‡¶ï‡¶Æ ‡¶π‡ßü ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶≠‡¶æ‡¶≤‡ßã ‡•§ ‡¶≤‡¶∏ ‡¶ï‡ßá minimize ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ neural network ‡¶è‡¶∞ parameter ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶ó‡ßÅ‡¶≤‡ßã change ‡¶ï‡¶∞‡¶ø ‡•§ ` 

- loss function ‡¶è‡¶ï‡¶ü‡¶æ mathematical function ‡¶Ø‡¶æ‡¶∞ input ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá neural network ‡¶è‡¶∞ parameter ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡ßá‡¶á ‡•§ 
- ‡¶Ü‡¶Æ‡¶∞‡¶æ Linear Regression problem solve ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶Ü‡¶Æ‡¶∞‡¶æ **mean_squared_error** ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ 


# Why loss function is important?
`You can't improve what you can't measure.` --üó£Ô∏è Peter Drucker

<br>

# How loss function is worked in neural network?
![Alt text](img/image-53.png)

<br>

Neural Network ‡¶è ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá weight and bias ‡¶è‡¶∞ random value ‡¶¶‡¶ø‡ßü‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶ø ‡•§ Neural Network train ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ student ‡¶è‡¶∞ info ‡¶ó‡ßÅ‡¶≤‡ßã neural network ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶ñ‡¶® `forward propagation` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶è‡¶∞ Y_output (prediction of the model) ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞, ‡¶Ü‡¶Æ‡¶∞‡¶æ `mean_squared_error` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá loss calculate ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞(loss) ‡¶™‡¶æ‡¶¨‡ßã ‡•§ ‡¶è‡¶á loss ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá `‡¶Ü‡¶Æ‡¶∞‡¶æ neural network ‡¶è‡¶∞ ‡¶™‡ßá‡¶õ‡¶®‡ßá ‡¶ó‡¶ø‡ßü‡ßá`  weight and bias ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã adjust ‡¶ï‡¶∞‡¶ø `gradient descent` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞, ‡¶Ü‡¶¨‡¶æ‡¶∞ Y_output (prediction of the model) ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá minimum loss ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø weight and bias ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã adjust ‡¶ï‡¶∞‡¶ø‡•§ 

<br>

# Type of loss function in deep learning:

![Alt text](img/image-54.png)

`keras ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶®‡¶ø‡¶ú‡ßá‡¶∞ loss function implement ‡¶ï‡¶∞‡¶æ‡¶∞ functionality provide ‡¶ï‡¶∞‡ßá ‡•§ `

<br>

# Loss Function vs Cost Function:

![Alt text](img/image-56.png)

`loss function ‡¶∂‡ßÅ‡¶ß‡ßÅ ‡¶è‡¶ï‡¶ü‡¶æ single dataset ‡¶è‡¶∞ entity ‡¶¨‡¶æ row  ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ, cost function(‡¶è‡¶∞ equation ‡¶õ‡¶¨‡¶ø‡¶§‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶Ü‡¶õ‡ßá) ‡¶™‡ßÅ‡¶∞‡ßã ‡¶è‡¶ï‡¶ü‡¶æ batch ‡¶¨‡¶æ ‡¶è‡¶ï‡¶ü‡¶æ epochs ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡•§ `

<br>



# Mean Squared Error Function:

- Use in regression problem

![Alt text](img/image-58.png)

- ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶ï‡ßá‡¶® square ‡¶ï‡¶∞‡¶ø? 
(y - y_out) ‡¶ï‡¶∞‡¶≤‡ßá difference negative ‡¶π‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡•§ square ‡¶ï‡¶∞‡¶æ ‡¶õ‡¶æ‡ßú‡¶æ ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶≤‡ßá negative ‡¶è‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá overall error ‡¶ï‡¶Æ‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá ‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ, ‡¶¨‡¶æ‡¶∏‡ßç‡¶§‡¶¨‡ßá ‡¶§‡ßã error value reduce ‡¶π‡¶¨‡ßá ‡¶®‡¶æ ‡•§ 

- Relation of error with bias and weight.
‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ square ‡¶ï‡¶∞‡¶õ‡¶ø ‡•§ ‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé, (y - y_out) ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶¨‡¶æ y_out ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶Ø‡¶§ ‡¶¨‡ßú ‡¶π‡¶¨‡ßá ‡•§ error ‡¶§‡¶§ magnify ‡¶π‡¶¨‡ßá ‡•§ error ‡¶Ø‡¶§ ‡¶¨‡ßú ‡¶π‡¶¨‡ßá weight and bias ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶§‡¶§ drastic ‡¶¨‡¶æ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶¨‡ßá  ‡¶¨‡¶æ weight and bias ‡¶è‡¶∞ ‡¶Ü‡¶™‡¶°‡ßá‡¶ü ‡¶§‡¶§ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡¶¨‡ßá ‡•§

<br>

### Advantage and Disadvantage of Mean_Squared_Error Loss Function:

![Alt text](img/image-59.png)

`Disadvantage:`
- outliers ‡¶ï‡ßá  fix ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Æ‡¶°‡ßá‡¶≤ ‡¶è‡¶∞ output ‡¶Ö‡¶®‡ßá‡¶ï ‡¶¨‡ßá‡¶∂‡¶ø ‡¶π‡ßü‡ßá ‡¶Ø‡¶æ‡¶¨‡ßá ‡•§ ‡¶Ø‡ßá‡¶á‡¶ü‡¶æ ‡¶ï‡¶æ‡¶Æ‡ßç‡¶Ø ‡¶®‡ßü ‡•§ ‡¶§‡¶æ‡¶á  outliears ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ mean_squared_error ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø ‡¶®‡¶æ ‡•§ 

`mean_squared_error ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∂‡¶∞‡ßç‡¶§ ‡¶π‡¶≤‡ßã ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶≤‡ßá‡ßü‡¶æ‡¶∞‡ßá‡¶∞ activation function ‡¶Ö‡¶¨‡ßç‡¶Ø‡¶∂‡¶á ‡¶Ö‡¶¨‡ßç‡¶Ø‡¶∂‡¶á linear ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§ `

<br>




# Mean Absolute Error:

- Use in regression problem

![Alt text](img/image-60.png)

- Mean_Squared_Error Loss Function ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã‡¶á ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ Square ‡¶è‡¶∞ ‡¶™‡¶∞‡¶¨‡¶∞‡ßç‡¶§‡¶ø‡¶§‡ßá absolute value ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ 
- Outliears ‡¶ï‡ßá ‡¶≠‡¶æ‡¶≤‡ßã‡¶≠‡¶æ‡¶¨‡ßá handle ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡•§  
- ‡¶è‡¶á‡¶ü‡¶æ ‡¶ï‡ßá differentiation ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü ‡¶®‡¶æ ‡•§ ‡¶Ü‡¶∞ ‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ, gradient decent ‡¶è‡¶ï‡¶¶‡¶Æ differentiation ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶®‡¶ø‡¶∞‡ßç‡¶≠‡¶∞‡¶∂‡ßÄ‡¶≤ ‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶∞‡¶æ gradient decent apply ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡¶ø ‡¶®‡¶æ ‡•§ ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, ‡¶Ü‡¶Æ‡¶∞‡¶æ sub-gradient decent apply ‡¶ï‡¶∞‡¶ø ‡•§  

<br>



# Huber Loss Function:

- Use in regression problem

‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ outliers ‡¶è‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ 25% ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡¶§‡¶æ‡¶π‡¶≤‡ßá ‡¶∏‡ßá‡¶á‡¶ó‡ßÅ‡¶≤‡ßã‡¶ï‡ßá ‡¶Ü‡¶∞ outliers ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ó‡¶£‡ßç‡¶Ø ‡¶ï‡¶∞‡¶æ ‡¶Ø‡¶æ‡ßü ‡¶®‡¶æ ‡•§ ‡¶è‡¶á ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ mean_squared_error function and mean_absolute_error ‡¶†‡¶ø‡¶ï ‡¶†‡¶æ‡¶ï ‡¶Æ‡¶§‡ßã ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡¶®‡¶æ ‡•§ 
‡¶§‡¶æ‡¶á ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ **huber loss function** ‡¶è‡¶∞ ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ú‡¶® ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡•§

![Alt text](img/image-61.png)

- outliers ‡¶®‡¶æ ‡¶•‡¶æ‡¶ï‡¶≤‡ßá Huber Loss Function -> mean_squared_error loss function ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡•§ 

- outliers ‡¶•‡¶æ‡¶ï‡¶≤‡ßá Huber Loss Function -> mean_absolute_error loss function ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá ‡¶ï‡¶æ‡¶ú ‡¶ï‡¶∞‡ßá ‡•§ 

`outliers ‡¶Ü‡¶õ‡ßá ‡¶ï‡¶ø ‡¶®‡¶æ ‡¶®‡ßá‡¶á ‡¶§‡¶æ ‡¶ó‡¶æ‡¶Æ‡¶æ(hyperparameter) ‡¶¶‡¶ø‡ßü‡ßá ‡¶®‡¶ø‡¶∞‡ßç‡¶ß‡¶æ‡¶∞‡¶£ ‡¶π‡¶¨‡ßá ‡•§ `
<br>



# Binary Cross Entrophy loss function ‡¶¨‡¶æ  log loss function:
 
-  Use in classification problem ‡•§

-  ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ class ‡¶•‡¶æ‡¶ï‡ßá ‡•§ (Binary classification problem)

![Alt text](img/image-62.png)

`Binary Cross Entrophy Loss Function ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∂‡¶∞‡ßç‡¶§ ‡¶π‡¶≤‡ßã ‡¶Ü‡¶â‡¶ü‡¶™‡ßÅ‡¶ü ‡¶≤‡ßá‡ßü‡¶æ‡¶∞‡ßá‡¶∞ activation function ‡¶Ö‡¶¨‡ßç‡¶Ø‡¶∂‡¶á ‡¶Ö‡¶¨‡ßç‡¶Ø‡¶∂‡¶á sigmoid ‡¶π‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡•§ `

- Cost function formula in Binary Cross Entrophy

![Alt text](img/image-63.png)

- `Cost function ‡¶è‡¶∞  working procedure:` Neural Network ‡¶è ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá weight and bias ‡¶è‡¶∞ random value ‡¶¶‡¶ø‡ßü‡ßá ‡¶∂‡ßÅ‡¶∞‡ßÅ ‡¶ï‡¶∞‡¶ø ‡•§ Neural Network train ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶∏‡¶Æ‡ßü ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶ñ‡¶® ‡¶è‡¶ï‡¶ü‡¶æ student ‡¶è‡¶∞ info ‡¶ó‡ßÅ‡¶≤‡ßã neural network ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶Ø‡¶æ‡ßü ‡¶§‡¶ñ‡¶® `forward propagation` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡¶è‡¶∞ Y_output (prediction of the model) ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞, ‡¶Ü‡¶Æ‡¶∞‡¶æ `binary_cross_entrophy` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá loss calculate ‡¶ï‡¶∞‡ßá ‡¶è‡¶ï‡¶ü‡¶æ ‡¶®‡¶æ‡¶Æ‡ßç‡¶¨‡¶æ‡¶∞(loss) ‡¶™‡¶æ‡¶¨‡ßã ‡•§ ‡¶è‡¶á loss ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞ ‡¶≠‡¶ø‡¶§‡ßç‡¶§‡¶ø ‡¶ï‡¶∞‡ßá `‡¶Ü‡¶Æ‡¶∞‡¶æ neural network ‡¶è‡¶∞ ‡¶™‡ßá‡¶õ‡¶®‡ßá ‡¶ó‡¶ø‡ßü‡ßá`  weight and bias ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã adjust ‡¶ï‡¶∞‡¶ø `gradient descent` ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶ß‡ßç‡¶Ø‡¶Æ‡ßá ‡•§ ‡¶§‡¶æ‡¶∞‡¶™‡¶∞, ‡¶Ü‡¶¨‡¶æ‡¶∞ Y_output (prediction of the model) ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá minimum loss ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø weight and bias ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã adjust ‡¶ï‡¶∞‡¶ø‡•§  


`Disadvantage:`
- multiple loacl minima ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡•§  

<br>



# Categorial Cross Entrophy Loss Function: 

-  Use in classification problem ‡•§

- ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ ‡¶è‡¶∞ ‡¶Ö‡¶ß‡¶ø‡¶ï class ‡¶•‡¶æ‡¶ï‡¶≤‡ßá ‡•§ (Multi-Classification Problem)

![Alt text](img/image-64.png)

- output layer ‡¶è ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã class ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡¶†‡¶ø‡¶ï ‡¶§‡¶§ ‡¶ó‡ßÅ‡¶≤‡ßã node ‡¶•‡¶æ‡¶ï‡¶¨‡ßá ‡•§ 
- Activation ‡¶π‡¶ø‡¶∏‡ßá‡¶¨‡ßá softmax activation function ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶¨‡ßã ‡•§ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶õ‡¶¨‡¶ø‡¶§‡ßá, Yes ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá e^z1 
- ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶õ‡¶¨‡¶ø‡¶§‡ßá, No ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá e^z2 
- ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶õ‡¶¨‡¶ø‡¶§‡ßá, Maybe ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶è‡¶∞ ‡¶â‡¶™‡¶∞‡ßá e^z3


<br>

- Cross Entrophy Loss Function ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá output ‡¶ï‡¶≤‡¶æ‡¶Æ‡ßá  One hot encoding ‡¶™‡ßç‡¶∞‡ßü‡ßã‡¶ó ‡¶ï‡¶∞‡¶ø ‡•§  

- dataset ‡¶•‡ßá‡¶ï‡ßá ‡ßÆ , ‡ßÆ‡ß¶ ‡¶á‡¶®‡¶™‡ßÅ‡¶ü ‡¶¶‡¶ø‡¶≤‡ßá, 
![Alt text](img/image-65.png)

‡¶Ü‡¶Æ‡¶∞‡¶æ output layer ‡¶è ‡ß© ‡¶ü‡¶æ ‡¶®‡ßã‡¶° ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡ß© ‡¶ü‡¶æ output ‡¶™‡¶æ‡¶¨‡ßã range to (0~1) ‡•§ ‡¶è‡¶ñ‡¶® ‡¶è‡¶á ‡¶§‡¶ø‡¶®‡¶ü‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶≤‡¶∏ ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡¶¨‡ßã‡•§ 

![Alt text](img/image-66.png)

‡¶§‡¶æ‡¶∞‡¶™‡¶∞, gradient decent apply ‡¶ï‡¶∞‡ßá weight and bias update ‡¶ï‡¶∞‡¶¨‡ßã  ‡•§  test data ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá, ‡¶§‡¶ø‡¶®‡¶ü‡¶æ‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶Ø‡¶æ‡¶∞ probability ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶¨‡ßá‡¶∂‡¶ø ‡¶Ü‡¶∏‡¶¨‡ßá ‡¶∏‡ßá‡¶á‡¶ü‡¶æ‡¶á ‡¶π‡¶¨‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶â‡¶§‡ßç‡¶§‡¶∞ ‡•§  

<br>



# Sparce Categorical Entrophy loss function:

**Sparce Categorial Entrophy and Categorical Cross Entrophy ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ ‡¶∏‡ßá‡¶Æ‡¶á ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶Ü‡¶Æ‡¶∞‡¶æ categorical cross entrophy ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ one hot encoding ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ ‡¶Ü‡¶∞ Sparce Categorical Entrophy ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ integer hot coding ‡¶ï‡¶∞‡¶ø ‡•§ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶è‡¶ï‡ßç‡¶∏‡¶æ‡¶Æ‡¶™‡ßã‡¶≤ ‡¶è integer hot coding ‡¶è yes no and maybe ‡¶ï‡ßá ‡¶§‡¶ø‡¶®‡¶ü‡¶æ integer assign ‡¶ï‡¶∞‡¶ø ‡•§**


![Alt text](img/image-67.png)

- Categorical Cross Entrophy formula ‡¶¶‡¶ø‡ßü‡ßá‡¶á  ‡¶ï‡¶∞‡¶¨‡ßã ‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ, ‡¶è‡¶ñ‡¶æ‡¶®‡ßá, ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ(row) ‡¶è ‡¶Ø‡ßá‡¶á ‡¶≤‡ßá‡¶≠‡ßá‡¶≤(1,2,3) ‡¶•‡¶æ‡¶ï‡¶¨‡ßá formula ‡¶§‡ßá ‡¶∏‡ßá‡¶á term ‡¶ü‡¶æ ‡¶õ‡¶æ‡ßú‡¶æ ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡ß¶ ‡¶¨‡¶∏‡¶ø‡ßü‡ßá ‡¶≤‡¶∏ ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶ï‡¶∞‡¶¨‡ßã ‡•§ 

### Formula of cost function:

![Alt text](img/image-68.png)


`Advantage:`
- ‡¶è‡¶á‡¶ü‡¶æ ‡¶§‡ßá ‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶°‡¶æ‡¶ü‡¶æ(row) ‡¶è ‡¶Ø‡ßá‡¶á ‡¶≤‡ßá‡¶≠‡ßá‡¶≤(1,2,3) ‡¶•‡¶æ‡¶ï‡¶¨‡ßá  formula ‡¶§‡ßá ‡¶∏‡ßá‡¶á term ‡¶ü‡¶æ ‡¶õ‡¶æ‡ßú‡¶æ ‡¶¨‡¶æ‡¶ï‡¶ø ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡ß¶ ‡¶¨‡¶∏‡¶ø‡ßü‡ßá ‡¶≤‡¶∏ ‡¶π‡¶ø‡¶∏‡¶æ‡¶¨ ‡¶§‡¶æ‡¶á ‡¶è‡¶á‡¶ü‡¶æ Categorical cross entrophy ‡¶è‡¶∞ ‡¶•‡ßá‡¶ï‡ßá fast ‡¶π‡ßü‡•§ 







