Click here for codes üëâüèª[click](https://github.com/AbuTaher003/Deep-Learning-DL/blob/main/Codes/06_Perceptron%20Loss%20Function%20%7C%20Hinge%20Loss%20%7C%20Binary%20Cross%20Entropy%20%7C%20Sigmoid%20Function.ipynb)
---

# Lost Function: (Lecture-06)


## Problem with perceptron Trick : 

![Alt text](img/image-19.png)

- 1st problem) ‡¶è‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ ‡¶≤‡¶æ‡¶á‡¶® ‡¶ê ‡¶†‡¶ø‡¶ï‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ ‡¶ï‡¶æ‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ï‡¶§  accurate ‡¶¨‡¶æ ‡¶ï‡ßã‡¶®‡¶ü‡¶æ better line  ‡¶§‡¶æ perceptron trick quantify ‡¶ï‡¶∞‡¶§‡ßá ‡¶™‡¶æ‡¶∞‡ßá ‡¶®‡¶æ ‡•§ 

![Alt text](img/image-20.png)

- problem 2) ‡¶è‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶π‡¶ö‡ßç‡¶õ‡ßá, ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡ßá‡¶á ‡ßß‡ß¶‡ß¶‡ß¶ ‡¶¨‡¶æ‡¶∞ ‡¶≤‡ßÅ‡¶™ ‡¶ò‡ßÅ‡¶∞‡¶æ‡¶¨‡ßã ‡¶∏‡ßá‡¶á ‡ßß‡ß¶‡ß¶‡ß¶ ‡¶¨‡¶æ‡¶∞  same point select ‡¶π‡ßü ‡•§ ‡¶Ø‡¶¶‡¶ø‡¶ì ‡¶è‡¶∞ ‡¶∏‡¶Æ‡ßç‡¶≠‡¶¨‡¶®‡¶æ ‡¶ñ‡ßÅ‡¶¨ ‡¶ï‡¶Æ ‡•§ 

‡¶è‡¶ñ‡¶® ‡¶∏‡¶¨‡¶ö‡ßá‡ßü‡ßá ‡¶≠‡¶æ‡¶≤‡ßã trick ‡¶π‡¶≤‡ßã perciption trick ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶®‡¶æ ‡¶ï‡¶∞‡ßá lost function ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶æ ‡•§ ‡¶∏‡¶π‡¶ó ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡•§ 

# Loss Function:  ( IN ALSO ML-> Machine Learning)

The loss function is a method of evaluating how well your machine learning algorithm models your featured data set. In other words, loss functions are a measurement of how good your model is in terms of predicting the expected outcome.

### What is loss function: 
Although there are different types of loss functions, fundamentally, they all operate by quantifying the difference between a **model's predictions** and the **actual target** value in the dataset. The official term for this numerical quantification is the **prediction error**. The learning algorithm and mechanisms in a machine learning model are optimized to minimize the prediction error, so this means that after a calculation of the value for the loss function, which is determined by the prediction error, the learning algorithm leverages this information to conduct weight and parameter updates which in effect during the next training pass leads to a lower prediction error.


![Alt text](img/image-21.png)


Like, ‡¶è‡¶ñ‡¶æ‡¶®‡ßá ‡¶è‡¶ï‡¶ü‡¶æ ‡¶´‡¶æ‡¶Ç‡¶∂‡¶® ‡¶¨‡¶æ‡¶®‡¶æ‡¶®‡ßã ‡¶Ü‡¶õ‡ßá ‡•§ ‡¶è‡¶∞ ‡¶Æ‡¶ß‡ßç‡¶Ø‡ßá ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ø‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ (W1,W2,b) provide ‡¶ï‡¶∞‡¶≤‡ßá ‡¶Ø‡¶æ‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ  prediction error ‡¶™‡¶æ‡¶¨‡ßã ‡•§  ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶õ‡¶¨‡¶ø‡¶§‡ßá ‡¶Ø‡¶æ‡¶∞ prediction error 23 ‡¶∏‡ßá‡¶á function ‡¶è‡¶∞ W1,W2,b ‡¶è‡¶∞ ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ø‡ßá ‡¶∞‡ßá‡¶ñ‡¶æ ‡¶™‡¶æ‡¶¨‡ßã ‡¶∏‡ßá‡¶á‡¶ü‡¶æ best fit ‡¶ï‡¶∞‡¶¨‡ßá ‡•§ 

`‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶∏‡¶Æ‡¶∏‡ßç‡¶Ø‡¶æ ‡¶∏‡¶Æ‡¶æ‡¶ß‡¶æ‡¶®‡ßá‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶¨‡¶ø‡¶≠‡¶ø‡¶®‡ßç‡¶® ‡¶ß‡¶∞‡¶®‡ßá‡¶∞ lost function ‡¶Ü‡¶õ‡ßá ‡•§ ml ‡¶è ‡¶Ü‡¶Æ‡¶∞‡¶æ I) Stochastic Gradient Descent II) Batch Gradient Descent III) Mini Batch Gradient Descent ‡•§ here, Gradient Descent = loss function . Perceptron ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ perceptron loss function ‡¶®‡¶ø‡ßü‡ßá ‡¶™‡ßú‡¶¨‡ßã ‡•§`



# Perceptron Loss Function: 

Loss Function is a mathematical function. ‡¶Ø‡ßá‡¶á‡¶ü‡¶æ ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶è‡¶ï‡¶ü‡¶æ number ‡¶¶‡¶ø‡¶¨‡ßá ‡•§ ‡¶∏‡ßá‡¶á‡¶ü‡¶æ ‡¶∏‡ßá‡¶á model ‡¶è‡¶∞ prediction error ‡•§ 

![Alt text](img/image-22.png)

‡¶è‡¶ñ‡¶æ‡¶®‡ßá, ‡ßß‡¶Æ ‡¶ö‡¶ø‡¶§‡ßç‡¶∞‡ßá ‡¶ï‡ßã‡¶® ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶â‡¶™‡¶∞  ‡¶Ø‡¶§‡¶ó‡ßÅ‡¶≤‡ßã misclassified point ‡¶Ü‡¶õ‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡¶¨‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶è‡¶ï ‡¶ß‡¶∞‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶Æ‡ßã‡¶ü prediction error  ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡¶Ø‡ßá‡¶á ‡¶ü‡¶æ ‡¶†‡¶ø‡¶ï ‡¶®‡¶æ ‡•§ ‡¶ï‡¶æ‡¶∞‡¶£, ‡¶Æ‡ßÇ‡¶≤‡¶¨‡¶ø‡¶®‡ßç‡¶¶‡ßÅ ‡¶•‡ßá‡¶ï‡ßá ‡¶Ø‡¶æ‡¶∞ ‡¶¶‡ßÇ‡¶∞‡¶§‡ßç‡¶¨ ‡¶¨‡ßá‡¶∂‡¶ø ‡¶§‡¶æ‡¶∞  prediction error ‡¶§‡¶§ ‡¶¨‡ßá‡¶∂‡¶ø ‡•§ 

‡¶§‡¶æ‡¶á, ‡¶°‡¶æ‡¶®‡¶™‡¶æ‡¶∂‡ßá‡¶∞ ‡¶ö‡¶ø‡¶§‡ßç‡¶∞‡ßá ‡¶ê ‡¶∞‡ßá‡¶ñ‡¶æ ‡¶•‡ßá‡¶ï‡ßá ‡¶§‡¶æ‡¶¶‡ßá‡¶∞ ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶¶‡ßÅ‡¶∞‡¶§‡ßç‡¶¨ ‡¶ï‡¶§ ‡¶§‡¶æ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡ßá ‡¶Ø‡ßã‡¶ó ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá‡•§ ‡¶Ø‡ßá‡¶π‡ßá‡¶§‡ßÅ ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶¶‡ßÅ‡¶∞‡¶§‡ßç‡¶¨ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ complex

![Alt text](img/image-23.png)

‡¶§‡¶æ‡¶á, ‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶≤‡¶æ‡¶á‡¶®‡ßá cordinate ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶ó‡ßÅ‡¶≤‡ßã ‡¶¨‡¶∏‡¶ø‡ßü‡ßá prediction error calculation ‡¶ï‡¶∞‡¶ø ‡•§ ‡¶è‡¶á‡¶ü‡¶æ ‡¶Ü‡¶∞ ‡¶∞‡ßá‡¶ñ‡¶æ‡¶∞ ‡¶•‡ßá‡¶ï‡ßá ‡¶≤‡¶Æ‡ßç‡¶¨ ‡¶¶‡ßÅ‡¶∞‡¶§‡ßç‡¶¨ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶æ proportional. Overall, ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶ó‡ßÅ‡¶≤‡ßã‡¶∞ mod ‡¶®‡¶ø‡ßü‡ßá negative value ‡¶Ü‡¶∏‡¶≤‡ßá ‡•§ 


# Documentation in sk-learn Stochastic Gradient Descent

![Alt text](img/image-25.png)

‡¶è‡¶ñ‡¶æ‡¶®‡ßá, L(loss) ‡¶¶‡¶ø‡ßü‡ßá ‡¶è‡¶ï‡¶ü‡¶æ term ‡¶Ü‡¶õ‡ßá ‡¶Ü‡¶∞ R (Regularization) ‡¶¶‡¶ø‡ßü‡ßá ‡¶Ü‡¶∞‡ßá‡¶ï‡¶ü‡¶æ term ‡¶Ü‡¶õ‡ßá ‡•§ 
perceptron ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø L(loss) function ‡¶ü‡¶æ ‡¶ï‡ßá‡¶Æ‡¶® ‡¶π‡¶¨‡ßá ‡¶§‡¶æ ‡¶®‡¶ø‡¶ö‡ßá ‡¶®‡¶ø‡¶ö‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶Ü‡¶õ‡ßá ‡•§ perceptron ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø L function ‡¶Ö‡¶®‡ßá‡¶ï ‡¶ü‡¶æ Hinge loss function ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã‡¶á ‡•§ ‡¶ï‡¶ø‡¶®‡ßç‡¶§‡ßÅ, Hinge loss function ‡¶è ‡ßß-similar term ‡¶Ü‡¶õ‡ßá ‡•§ 

![Alt text](img/image-26.png)


‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶Ü‡¶™‡¶æ‡¶§‡¶§ regularization ‡¶ï‡ßá ignore ‡¶ï‡¶∞‡¶¨‡ßã‡•§ L(loss) function ‡¶è f(x_i) ‡¶π‡¶≤‡ßá ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶∏‡ßá‡¶á ‡¶≤‡¶æ‡¶á‡¶® ‡¶ü‡¶æ ‡¶¨‡¶æ ‡¶Æ‡¶°‡ßá‡¶≤‡¶ü‡¶æ ‡¶Ü‡¶∞ y_i ‡¶π‡¶≤‡ßã output iq,cgpa ‡¶è‡¶∞ example ‡¶è‡¶∞ ‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá placedment ‡¶π‡¶ö‡ßç‡¶õ‡ßá y_i ‡¶Ü‡¶∞ n ‡¶π‡¶ö‡ßç‡¶õ‡ßá number of row in dataset ‡•§ 

`‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ (W_1,W_2,b) ‡¶è‡¶∞ ‡¶è‡¶Æ‡¶® ‡¶≠‡ßç‡¶Ø‡¶æ‡¶≤‡ßÅ ‡¶¨‡ßá‡¶∞ ‡¶ï‡¶∞‡¶§‡ßá ‡¶π‡¶¨‡ßá ‡¶Ø‡¶æ‡¶∞ ‡¶ï‡¶æ‡¶∞‡¶£‡ßá L(loss) function ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶Æ‡¶ø‡¶®‡¶ø‡¶Æ‡¶æ‡¶Æ ‡¶Ü‡¶∏‡¶¨‡ßá‡•§ ‡¶è‡¶∞ ‡¶ú‡¶®‡ßç‡¶Ø ‡¶Ü‡¶Æ‡¶∞‡¶æ gradient decent ‡¶¨‡ßç‡¶Ø‡¶¨‡¶π‡¶æ‡¶∞ ‡¶ï‡¶∞‡¶ø ‡•§ ` 

# Explanation of Loss Function:

![Alt text](img/image-27.png)

‡¶Ü‡¶Æ‡¶∞‡¶æ ‡¶™‡ßç‡¶∞‡¶•‡¶Æ‡ßá ‡¶è‡¶á table ‡¶ü‡¶æ‡¶∞ explanation ‡¶¶‡ßá‡¶ñ‡¶ø ‡•§ Table ‡¶ü‡¶æ matrix ‡¶è‡¶∞ ‡¶Æ‡¶§‡ßã ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá, element ‡¶ó‡ßÅ‡¶≤‡ßã X_ij ‡¶¶‡¶ø‡ßü‡ßá representation ‡¶ï‡¶∞‡¶æ ‡¶π‡ßü‡ßá‡¶õ‡ßá ‡•§ ‡¶Ø‡ßá‡¶ñ‡¶æ‡¶®‡ßá, i ‡¶π‡¶ö‡ßç‡¶õ‡ßá row ‡¶Ü‡¶∞ j ‡¶π‡¶ö‡ßç‡¶õ‡ßá column ‡•§ 

![Alt text](img/image-28.png)

‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ ‡¶ï‡¶æ‡¶õ‡ßá ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ datapoint ‡¶Ü‡¶õ‡ßá ‡¶Ö‡¶∞‡ßç‡¶•‡¶æ‡ßé ‡¶Ü‡¶Æ‡¶æ‡¶¶‡ßá‡¶∞ n = 2 ‡¶∏‡ßá‡¶ï‡ßç‡¶∑‡ßá‡¶§‡ßç‡¶∞‡ßá, L ‡¶è‡¶∞ ‡¶Æ‡¶æ‡¶® ‡¶¶‡ßÅ‡¶á‡¶ü‡¶æ ‡¶π‡¶¨‡ßá ‡•§ f(x1) = w1X11 + w2 X12 + b ‡•§ f(x2) ‡¶è‡¶∞ ‡¶ü‡¶æ ‡¶â‡¶™‡¶∞‡ßá‡¶∞ ‡¶õ‡¶¨‡¶ø‡¶§‡ßá ‡¶¶‡ßá‡¶ì‡ßü‡¶æ ‡¶Ü‡¶õ‡ßá ‡•§ 

<br>
Example: <br>

![Alt text](img/image-29.png)


