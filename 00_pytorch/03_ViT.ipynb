{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# `Architecture of ViT: `\n",
    "\n",
    "<br>\n",
    "\n",
    "![image_image](img/img.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **ইনপুট চিত্রের বিভাজন (Image Partitioning):**\n",
    "\n",
    "- Input Picture কে ছোট ছোট প্যাচে `(Patch বলতে, ছবিটা ভাগ করা হয় । আমরা উপরের ছবিতে ইচ্ছে করলে ৪ ভাগে ভাগ করতে পারবো)` বিভক্ত করা হয়। উদাহরণস্বরূপ, একটি  224×224 চিত্রকে 16×16 পিক্সেলের প্যাচে ভাগ করলে, আমরা ছবিতে কে ১৪ টুকরা করতে পারবো 224x224 = 14*(16x16) । \n",
    "\n",
    "- প্রতিটি প্যাচকে Flatten করা হয় এবং একটি ভেক্টরে রূপান্তর করা হয়। আমরা যখন, transformer পড়েছি, তখন, দেখেছি এইটা 1D data input নেয় । \n",
    "\n",
    "2. **প্যাচ এম্বেডিং (Patch Embedding):**\n",
    "\n",
    "- প্রতিটি প্যাচ ভেক্টরের উপর লিনিয়ার প্রজেকশন(Linear Projection) অ্যাপ্লাই করা হয়, যা প্যাচের বৈশিষ্ট্য ধারণ করে।\n",
    "- এরপর প্যাচগুলোর সাথে পজিশন এম্বেডিং যোগ করা হয়। পজিশন এম্বেডিং চিত্রের প্যাচগুলোর ক্রম ধরে রাখার জন্য ব্যবহৃত হয়। `Position Embedding simillar to Positional Encoding in Transformer.`\n",
    "\n",
    "### `এখানে, Linear Projection কী?`\n",
    "\n",
    "আগে আমরা উপরের architecture এর সাথে related আমরা কিছু mathmatical equation দেখি। তারপর Linear Projection সম্পর্কে জানবো । \n",
    "\n",
    "\n",
    "![image](img/img01.png)\n",
    "\n",
    "\n",
    "`উপরের প্রথম equation এ`,\n",
    " \n",
    "- $X_{p}^1$ হচ্ছে ছবির ১ম টুকরা (1st patch) । \n",
    "- $X_{p}^2$ হচ্ছে ছবির ২য় টুকরা (2nd patch) । \n",
    "- $X_{p}^3$ হচ্ছে ছবির ৩য় টুকরা (3rd patch) । \n",
    "\n",
    "**`এখন, Equation এ E কি?`**\n",
    "\n",
    "E হচ্ছে Embedding আর আমরা **Linear Projection** এর মাধ্যমে এই Embedding বানিয়ে থাকি । **Linear Projection** মানে হলো $X_{p1}$ -এর ফ্ল্যাটেন করা ভেক্টরটি একটি **Feed-Forward Neural Network (FFNN)**-এ দেওয়া হয়।  এই FFNN আসলে একটি **Fully Connected Layer**, যেটি $X_{p1}$ -এর ইনপুট ভেক্টরকে প্রজেক্ট করে একটি নির্দিষ্ট ডাইমেনশনের **Embedding Vector**-এ রূপান্তর করে। এই প্রজেকশনের মাধ্যমে $X_{p1}$ -এর **Feature Representation** তৈরি করা হয়, যেটি পরবর্তী ট্রান্সফরমার ব্লকে প্রসেস করা হয়। \n",
    "\n",
    "**`এখানে, [class] কি?`**\n",
    "আমরা তো ছবি কে কয়েক ভাবে ভাগ করে ফেলতেছি । কিন্তু, ছবিটা কীসের ছিল সেইটা তো আমাদের জানা দরকার আর সেই information থাকে আমাদের [class] token এ । \n",
    "\n",
    "**`এর পর,`**\n",
    "- MSA -> Multi Head Self Attention.\n",
    "- MLP -> Multi Layer Perceptron.\n",
    "- LN -> Layer Normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `Difference between MSA(Multi Head Self Attention) vs MHA(Multi Head Attention): `\n",
    "\n",
    "\n",
    "### **1. Multi-Head Self-Attention (MSA):**\n",
    "\n",
    "- `Transfomer এর Encoder এ আমরা সেম sequence (যেমনঃ English Language ) জন্য নিজেদের মধ্যে সম্পর্ক বের করেছিলাম ।  যেখানে **Query (Q)**, **Key (K)**, এবং **Value (V)** এদের উৎস একটাই ছিল । এর জন্য একে, Multi Head Self Attention বলে । `\n",
    "\n",
    "### **2. Multi-Head Attention (MHA):**\n",
    "- `Multi-Head Attention হলো একটি সাধারণ Attention Mechanism, যেখানে **Query** ইনপুট আসে এক উৎস থেকে এবং **Key, Value** আসে আরেক উৎস থেকে। যেইটা আমরা Transformer এর decoder এ দেখেছিলাম । আমরা যাকে **cross attention** নামে জেনেছিলাম । `\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `What is the role of Embedding?`\n",
    "\n",
    "\n",
    "### **১. ছবিকে প্যাচে ভাগ করা:**\n",
    "- Research paper এ **224x224 আকারের রঙিন ছবি** নেওয়া হয়েছে । \n",
    "- Color image 3 Channel. For 8 bit image (255x255x255)= 16581375. We can represent 16581375 color with RGB image. For, example if we want to make silver color in rgb color space then, silver is made of 75% red, 75% green, and 75% blue. \n",
    "- প্রতিটি প্যাচের সাইজ **16x16x3 (H x W x C)**।  \n",
    "- পুরো ছবিকে **196 প্যাচে ভাগ করা যাবে**, কারণ:  \n",
    "  \n",
    "  $\\frac{224 \\times 224}{16 \\times 16}$ = 196\n",
    "  \n",
    "\n",
    "###  **২. প্রতিটি প্যাচকে ফ্ল্যাট(Flatten) করা:**\n",
    "- প্রতিটি প্যাচের ডাইমেনশন: $16 \\times 16 \\times 3$ (যেটা Color image ছবির জন্য H × W × C)।  \n",
    "- এই প্যাচকে ফ্ল্যাট করলে, একে ১D ভেক্টর করা হয়:\n",
    "\n",
    "  $16 \\times 16 \\times 3$ = 768\n",
    "\n",
    "\n",
    "\n",
    "### **৩. Linear Projection এর মাধ্যমে Embedding তৈরি:**\n",
    "- প্রতিটি ফ্ল্যাট করা প্যাচকে একটি **Fully Connected Neural Network (Linear Layer)**-এ দেওয়া হয়।  \n",
    "- এই লেয়ারের কাজ হলো প্রতিটি প্যাচ থেকে একটি **Feature Embedding (768-ডাইমেনশন)**(16x16x3)=768 তৈরি করা।  \n",
    "- **এই 768-ডাইমেনশন ফিচার ভেক্টরটি প্যাচটির ভেতরে কী আছে তা রিপ্রেজেন্ট করে।**\n",
    "\n",
    "\n",
    "### **৪. 196 প্যাচ থেকে 196 Embedding:**\n",
    "- পুরো 196 প্যাচের জন্য একই প্রসেস চালানো হয়।  \n",
    "- ফলে, আমরা **196টি Embedding** পাই, এবং প্রতিটির ডাইমেনশন 768।  \n",
    "- এই 196 Embedding সমষ্টিগতভাবে পুরো ছবির তথ্য বহন করে।  \n",
    "\n",
    "### **ViT-এর জন্য এটি কেন গুরুত্বপূর্ণ?**\n",
    "এই Embedding গুলো ট্রান্সফরমারের ইনপুট হিসেবে ব্যবহৃত হয়, যেখানে **Self-Attention Mechanism** ছবির বিভিন্ন অংশের মধ্যে সম্পর্ক বোঝার কাজ করে। \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
