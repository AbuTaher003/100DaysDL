{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "\n",
    "# `Architecture of ViT: `\n",
    "\n",
    "<br>\n",
    "\n",
    "![image_image](img/img.png)\n",
    "\n",
    "<br>\n",
    "\n",
    "1. **ইনপুট চিত্রের বিভাজন (Image Partitioning):**\n",
    "\n",
    "- Input Picture কে ছোট ছোট প্যাচে `(Patch বলতে, ছবিটা ভাগ করা হয় । আমরা উপরের ছবিতে ইচ্ছে করলে ৪ ভাগে ভাগ করতে পারবো)` বিভক্ত করা হয়। উদাহরণস্বরূপ, একটি  224×224 চিত্রকে 16×16 পিক্সেলের প্যাচে ভাগ করলে, আমরা ছবিতে কে ১৪ টুকরা করতে পারবো 224x224 = 14*(16x16) । \n",
    "\n",
    "- প্রতিটি প্যাচকে Flatten করা হয় এবং একটি ভেক্টরে রূপান্তর করা হয়। আমরা যখন, transformer পড়েছি, তখন, দেখেছি এইটা 1D data input নেয় । \n",
    "\n",
    "2. **প্যাচ এম্বেডিং (Patch Embedding):**\n",
    "\n",
    "- প্রতিটি প্যাচ ভেক্টরের উপর লিনিয়ার প্রজেকশন(Linear Projection) অ্যাপ্লাই করা হয়, যা প্যাচের বৈশিষ্ট্য ধারণ করে।\n",
    "- এরপর প্যাচগুলোর সাথে পজিশন এম্বেডিং যোগ করা হয়। পজিশন এম্বেডিং চিত্রের প্যাচগুলোর ক্রম ধরে রাখার জন্য ব্যবহৃত হয়। `Position Embedding simillar to Positional Encoding in Transformer.`\n",
    "\n",
    "### `এখানে, Linear Projection কী?`\n",
    "\n",
    "আগে আমরা উপরের architecture এর সাথে related আমরা কিছু mathmatical equation দেখি। তারপর Linear Projection সম্পর্কে জানবো । \n",
    "\n",
    "\n",
    "![image](img/img01.png)\n",
    "\n",
    "\n",
    "`উপরের প্রথম equation এ`,\n",
    " \n",
    "- $X_{p}^1$ হচ্ছে ছবির ১ম টুকরা (1st patch) । \n",
    "- $X_{p}^2$ হচ্ছে ছবির ২য় টুকরা (2nd patch) । \n",
    "- $X_{p}^3$ হচ্ছে ছবির ৩য় টুকরা (3rd patch) । \n",
    "\n",
    "**`এখন, Equation এ E কি?`**\n",
    "\n",
    "E হচ্ছে Embedding আর আমরা **Linear Projection** এর মাধ্যমে এই Embedding বানিয়ে থাকি । **Linear Projection** মানে হলো $X_{p1}$ -এর ফ্ল্যাটেন করা ভেক্টরটি একটি **Feed-Forward Neural Network (FFNN)**-এ দেওয়া হয়।  এই FFNN আসলে একটি **Fully Connected Layer**, যেটি $X_{p1}$ -এর ইনপুট ভেক্টরকে প্রজেক্ট করে একটি নির্দিষ্ট ডাইমেনশনের **Embedding Vector**-এ রূপান্তর করে। এই প্রজেকশনের মাধ্যমে $X_{p1}$ -এর **Feature Representation** তৈরি করা হয়, যেটি পরবর্তী ট্রান্সফরমার ব্লকে প্রসেস করা হয়। \n",
    "\n",
    "**`এখানে, [class] কি?`**\n",
    "আমরা তো ছবি কে কয়েক ভাবে ভাগ করে ফেলতেছি । কিন্তু, ছবিটা কীসের ছিল সেইটা তো আমাদের জানা দরকার আর সেই information থাকে আমাদের [class] token এ । \n",
    "\n",
    "**`এর পর,`**\n",
    "- MSA -> Multi Head Self Attention.\n",
    "- MLP -> Multi Layer Perceptron.\n",
    "- LN -> Layer Normalization.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `Difference between MSA(Multi Head Self Attention) vs MHA(Multi Head Attention): `\n",
    "\n",
    "\n",
    "### **1. Multi-Head Self-Attention (MSA):**\n",
    "\n",
    "- `Transfomer এর Encoder এ আমরা সেম sequence (যেমনঃ English Language ) জন্য নিজেদের মধ্যে সম্পর্ক বের করেছিলাম ।  যেখানে **Query (Q)**, **Key (K)**, এবং **Value (V)** এদের উৎস একটাই ছিল । এর জন্য একে, Multi Head Self Attention বলে । `\n",
    "\n",
    "### **2. Multi-Head Attention (MHA):**\n",
    "- `Multi-Head Attention হলো একটি সাধারণ Attention Mechanism, যেখানে **Query** ইনপুট আসে এক উৎস থেকে এবং **Key, Value** আসে আরেক উৎস থেকে। যেইটা আমরা Transformer এর decoder এ দেখেছিলাম । আমরা যাকে **cross attention** নামে জেনেছিলাম । `\n",
    "\n",
    "<br>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "## `What is the role of Embedding?`\n",
    "\n",
    "\n",
    "### **১. ছবিকে প্যাচে ভাগ করা:**\n",
    "- Research paper এ **224x224 আকারের রঙিন ছবি** নেওয়া হয়েছে । \n",
    "- Color image 3 Channel. For 8 bit image (255x255x255)= 16581375. We can represent 16581375 color with RGB image. For, example if we want to make silver color in rgb color space then, silver is made of 75% red, 75% green, and 75% blue. \n",
    "- প্রতিটি প্যাচের সাইজ **16x16x3 (H x W x C)**।  \n",
    "- পুরো ছবিকে **196 প্যাচে ভাগ করা যাবে**, কারণ:  \n",
    "  \n",
    "  $\\frac{224 \\times 224}{16 \\times 16}$ = 196\n",
    "  \n",
    "\n",
    "###  **২. প্রতিটি প্যাচকে ফ্ল্যাট(Flatten) করা:**\n",
    "- প্রতিটি প্যাচের ডাইমেনশন: $16 \\times 16 \\times 3$ (যেটা Color image ছবির জন্য H × W × C)।  \n",
    "- এই প্যাচকে ফ্ল্যাট করলে, একে ১D ভেক্টর করা হয়:\n",
    "\n",
    "  $16 \\times 16 \\times 3$ = 768\n",
    "\n",
    "\n",
    "\n",
    "### **৩. Linear Projection এর মাধ্যমে Embedding তৈরি:**\n",
    "- প্রতিটি ফ্ল্যাট করা প্যাচকে একটি **Fully Connected Neural Network (Linear Layer)**-এ দেওয়া হয়।  \n",
    "- এই লেয়ারের কাজ হলো প্রতিটি প্যাচ থেকে একটি **Feature Embedding (768-ডাইমেনশন)**(16x16x3)=768 তৈরি করা।  \n",
    "- **এই 768-ডাইমেনশন ফিচার ভেক্টরটি প্যাচটির ভেতরে কী আছে তা রিপ্রেজেন্ট করে।**\n",
    "\n",
    "\n",
    "### **৪. 196 প্যাচ থেকে 196 Embedding:**\n",
    "- পুরো 196 প্যাচের জন্য একই প্রসেস চালানো হয়।  \n",
    "- ফলে, আমরা **196টি Embedding** পাই, এবং প্রতিটির ডাইমেনশন 768।  \n",
    "- এই 196 Embedding সমষ্টিগতভাবে পুরো ছবির তথ্য বহন করে।  \n",
    "\n",
    "### **ViT-এর জন্য এটি কেন গুরুত্বপূর্ণ?**\n",
    "এই Embedding গুলো ট্রান্সফরমারের ইনপুট হিসেবে ব্যবহৃত হয়, যেখানে **Self-Attention Mechanism** ছবির বিভিন্ন অংশের মধ্যে সম্পর্ক বোঝার কাজ করে। \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "Let's Understand more. \n",
    "\n",
    "ধরো, আমি বিয়ে করবো, তো বিয়ে করার জন্য ৩ টা মেয়ের ছবি আছে । এখন আমি যদি জানতে চায়, কোন মেয়েটা  সবচেয়ে সুন্দর, তার চোখ, কান, নাক , গলা, কোমর ইত্যাদি ইত্যাদি । তো আমি যদি ছবি ৩টাকে একটা ViT এ পাঠায় তাহলে, আমরা প্রথমে তো, ছবি গুলোকে প্যাচে convert করবো । তারপর, এই গুলো থেকে linear projection এর মাধ্যমে আমরা যেই embedding বানাবো সেখানে, আলাদা আলাদা patch এ যদি চোখ, কান, নাক , গলা, কোমর যায় তাহলে এদের জন্য যেই 786 dimention এর embedding বানাবো যেখানে কিছু feature make হয়ে সেই অনুয়ায়ী value বসে, সব patch এর ভিত্তিতে কোন মেয়েটা সবচেয়ে সুন্দর তার value পেয়ে যাবো ।   \n",
    "\n",
    "তোমার গল্পটা খুবই মজার এবং দারুণভাবে ViT-এর কাজ করার পদ্ধতিকে বাস্তব উদাহরণের মাধ্যমে ব্যাখ্যা করেছে! তবে, এখানে কিছু বিষয় একটু বিশদভাবে ব্যাখ্যা করলে আরও পরিষ্কার হবে। আসো, বিস্তারিত দেখি:\n",
    "\n",
    "\n",
    "### **গল্পটি কতটা ঠিক?**\n",
    "\n",
    "তোমার গল্পের মূল ধারণা **ঠিক আছে**, তবে বাস্তব মডেলিংয়ে কিছু বাড়তি বিষয় থাকতে পারে:\n",
    "\n",
    "1. **চোখ, কান, নাক, গলা ইত্যাদির ফিচার রিপ্রেজেন্টেশন:**\n",
    "   - একেকটি প্যাচে **চোখ বা নাক** থাকার সম্ভাবনা বেশি। প্যাচের Embedding-এ চোখের আকার, রঙ, এবং অবস্থানের মতো বৈশিষ্ট্যগুলো ধরা পড়বে।\n",
    "   - ViT-এর Self-Attention এই ফিচারগুলো বিশ্লেষণ করে বোঝে যে চোখ, নাক, গলা ইত্যাদির মধ্যে কোনটি সবচেয়ে বেশি \"সুন্দর\" বা গুরুত্বপূর্ণ।  \n",
    "\n",
    "2. **Patch-Based Analysis:**\n",
    "   - ViT সরাসরি বলে না যে \"এই প্যাচে চোখ আছে\"। তবে, **Attention Mechanism** এবং **Learned Embedding** থেকে মডেল এটা বুঝতে পারে।  \n",
    "   - শেষের দিকে **Classification Head** এই ফিচারগুলোকে কাজে লাগিয়ে পুরো ছবির জন্য একটি সৌন্দর্য স্কোর তৈরি করে।\n",
    "\n",
    "3. **সৌন্দর্যের স্কোর বের করা:**\n",
    "   - সৌন্দর্য একটি সাবজেক্টিভ ধারণা। যদি তুমি মডেলকে ট্রেন করো এমনভাবে যে, চোখ, নাক, গলা, কোমর ইত্যাদির জন্য **সৌন্দর্য নির্ধারণের কিছু মানদণ্ড** শেখানো হয় (যেমন: চোখ বড় হলে সুন্দর), তাহলে মডেল সৌন্দর্য বিচার করতে পারবে।\n",
    "\n",
    "\n",
    "### **গল্পের মূল্যায়ন**\n",
    "তোমার গল্পটি পুরোপুরি সঠিক পথে আছে। ViT প্যাচ-ভিত্তিক ফিচার রিপ্রেজেন্টেশন এবং Attention ব্যবহার করে ছবির প্রতিটি অংশের সৌন্দর্যের গুরুত্ব নির্ধারণ করতে পারে। আর মডেলের **Self-Attention Mechanism** মেয়েদের **চোখ, নাক, গলা** ইত্যাদির মধ্যে সম্পর্ক এবং তাদের গুরুত্ব বুঝে কাজ করে।\n",
    "\n",
    "\n",
    "---\n",
    "<br>\n",
    "\n",
    "---\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![image](img/img02.png)\n",
    "![image](img/img03.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "code:"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
