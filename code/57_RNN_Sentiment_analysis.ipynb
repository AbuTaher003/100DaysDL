{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "\n",
    "# Convert text into `Vector:`\n",
    "\n",
    "- integer encoding \n",
    "- embeddings\n",
    "\n",
    "### Now, we will see `integer encoding: `\n",
    "\n",
    "![Alt text](image-1.png)\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "---\n",
    "---\n",
    "\n",
    "<br>\n",
    "\n",
    "# code :\n",
    "\n",
    "<br>\n",
    "\n",
    "---\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "import matplotlib.pyplot as plt \n",
    "import tensorflow as tf "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = [ 'go india',\n",
    "'india india',\n",
    "'hip hip hurray',\n",
    "'jeetega bhai jeetega india jeetega',\n",
    "'bharat mata ki jai',\n",
    "'kohli kohli',\n",
    "'sachin sachin',\n",
    "'dhoni dhoni',\n",
    "'modi ji ki jai',\n",
    "'inquilab zindabad'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# now we need to tokenize the docs: \n",
    "# - tokenizer take word by word then letter by letter then,\n",
    "# - if we have capital letter what will be remove \n",
    "# - remove special symbol . \n",
    "\n",
    "tokinizer = tf.keras.preprocessing.text.Tokenizer(oov_token='<<nothing>>') # when we test our model \n",
    "# if any word come that is not in my docs then it will replace with nothing word.\n",
    "# oov -> out of vocabulary(docs)\n",
    "\n",
    "tokinizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{1: '<<nothing>>',\n",
       " 2: 'india',\n",
       " 3: 'jeetega',\n",
       " 4: 'hip',\n",
       " 5: 'ki',\n",
       " 6: 'jai',\n",
       " 7: 'kohli',\n",
       " 8: 'sachin',\n",
       " 9: 'dhoni',\n",
       " 10: 'go',\n",
       " 11: 'hurray',\n",
       " 12: 'bhai',\n",
       " 13: 'bharat',\n",
       " 14: 'mata',\n",
       " 15: 'modi',\n",
       " 16: 'ji',\n",
       " 17: 'inquilab',\n",
       " 18: 'zindabad'}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinizer.index_word # show only the unique words with index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('go', 1),\n",
       "             ('india', 4),\n",
       "             ('hip', 2),\n",
       "             ('hurray', 1),\n",
       "             ('jeetega', 3),\n",
       "             ('bhai', 1),\n",
       "             ('bharat', 1),\n",
       "             ('mata', 1),\n",
       "             ('ki', 2),\n",
       "             ('jai', 2),\n",
       "             ('kohli', 2),\n",
       "             ('sachin', 2),\n",
       "             ('dhoni', 2),\n",
       "             ('modi', 1),\n",
       "             ('ji', 1),\n",
       "             ('inquilab', 1),\n",
       "             ('zindabad', 1)])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinizer.word_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "10"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokinizer.document_count # see all the number of rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[10, 2],\n",
       " [2, 2],\n",
       " [4, 4, 11],\n",
       " [3, 12, 3, 2, 3],\n",
       " [13, 14, 5, 6],\n",
       " [7, 7],\n",
       " [8, 8],\n",
       " [9, 9],\n",
       " [15, 16, 5, 6],\n",
       " [17, 18]]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence = tokinizer.texts_to_sequences(docs) # convert text to sequence\n",
    "sequence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0,  0,  0, 10,  2],\n",
       "       [ 0,  0,  0,  2,  2],\n",
       "       [ 0,  0,  4,  4, 11],\n",
       "       [ 3, 12,  3,  2,  3],\n",
       "       [ 0, 13, 14,  5,  6],\n",
       "       [ 0,  0,  0,  7,  7],\n",
       "       [ 0,  0,  0,  8,  8],\n",
       "       [ 0,  0,  0,  9,  9],\n",
       "       [ 0, 15, 16,  5,  6],\n",
       "       [ 0,  0,  0, 17, 18]], dtype=int32)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# now do padding\n",
    "final = tf.keras.utils.pad_sequences(sequences=sequence,padding='pre')\n",
    "\n",
    "final\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venvML",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
